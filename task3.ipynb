{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtJ4FthwWpBq",
        "colab_type": "text"
      },
      "source": [
        "# Task3: MPNN-based Quark/Gluon Jet Classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h5FQCfCYOZs",
        "colab_type": "text"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtx7PK0Q-GOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WabGmLthE0TC",
        "colab_type": "code",
        "outputId": "2b058946-691d-4796-c2ea-e7252d55480f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmpJruccBNeH",
        "colab_type": "code",
        "outputId": "cedb9051-9d2e-4117-b172-b3369b4bd97a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = np.load('/content/drive/My Drive/QG_jets.npz')\n",
        "data.files"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['X', 'y']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca54jzsvBVgB",
        "colab_type": "code",
        "outputId": "b3643081-c759-42d8-dc1b-06b467872375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "features = data['X']\n",
        "labels = data['y']\n",
        "print(\"features shape:\", features.shape)\n",
        "print(\"labels shape\", labels.shape)\n",
        "print(\"features:\", features[0])\n",
        "print(\"labels\", labels[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features shape: (100000, 139, 4)\n",
            "labels shape (100000,)\n",
            "features: [[ 2.68769142e-01  3.56903171e-01  4.74138734e+00  2.20000000e+01]\n",
            " [ 1.60076377e-01 -2.55609533e-01  4.55022910e+00  2.20000000e+01]\n",
            " [ 1.14868731e+00 -6.24380156e-02  4.50385377e+00 -2.11000000e+02]\n",
            " [ 4.13159146e+00  1.73686350e-01  4.76622410e+00 -3.21000000e+02]\n",
            " [ 1.69599701e+00 -2.12177764e-01  4.79687162e+00 -2.11000000e+02]\n",
            " [ 2.19372581e+00 -5.24780791e-02  4.57559636e+00  2.20000000e+01]\n",
            " [ 1.61909680e+00 -6.76247614e-02  4.64561192e+00  2.20000000e+01]\n",
            " [ 6.59214883e+00  4.42691311e-02  4.76597141e+00  2.11000000e+02]\n",
            " [ 3.77096258e+00  4.22475280e-02  4.75473207e+00  3.21000000e+02]\n",
            " [ 1.34816345e+01 -2.80005472e-02  4.73543183e+00 -2.11000000e+02]\n",
            " [ 4.10794493e+00 -2.37648715e-02  4.75891312e+00  2.20000000e+01]\n",
            " [ 2.16455176e+01 -2.69973695e-02  4.75997450e+00  2.20000000e+01]\n",
            " [ 6.77551168e+00 -2.97549224e-02  4.76127746e+00  2.20000000e+01]\n",
            " [ 1.32550803e+01 -3.94389998e-02  4.74948328e+00  2.20000000e+01]\n",
            " [ 2.98350842e+00 -3.66033986e-02  4.74619579e+00  2.20000000e+01]\n",
            " [ 3.73743866e+01 -3.54428448e-02  4.75481319e+00  2.20000000e+01]\n",
            " [ 3.35399046e+02 -3.25602518e-02  4.75085918e+00  2.20000000e+01]\n",
            " [ 4.42135649e+01 -3.32763050e-02  4.75431907e+00  2.20000000e+01]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "labels 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6NDt0GDYVhi",
        "colab_type": "text"
      },
      "source": [
        "## Feature Mapping and Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "55d43be9-c828-4a36-b0e4-765b0cc6f59e",
        "id": "pSdF9gfq2SqP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pdgid_class = np.unique(features[:,:,3])\n",
        "print(pdgid_class)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2212. -2112.  -321.  -211.   -13.   -11.     0.    11.    13.    22.\n",
            "   130.   211.   321.  2112.  2212.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLy9j9ul28IO",
        "colab_type": "code",
        "outputId": "f4436dbf-651b-4f57-8d23-e64c2d7c7163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "i = features[0]\n",
        "np.all(i == 0, axis=-1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PeufOTx2sT3",
        "colab_type": "code",
        "outputId": "da8cda61-2461-46fc-fa86-b5c55d29e280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "def avg_pt(features):\n",
        "  return np.array([np.mean(i[~np.all(i==0, axis=-1),0]) for i in features])\n",
        "\n",
        "def get_pdg_num(feature, pclass):\n",
        "    return np.count_nonzero(feature[~np.all(feature==0, axis=-1),3] == pclass)\n",
        "\n",
        "def pdg_num(features):\n",
        "  return np.array([[get_pdg_num(i, n) for n in (pdgid_class)] for i in features])\n",
        "\n",
        "average_pt = avg_pt(features)\n",
        "pdg_number = pdg_num(features)\n",
        "\n",
        "print(average_pt)\n",
        "print(pdg_number)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[27.82318056 31.28284093  8.89669563 ... 34.26094921  6.20155106\n",
            " 13.64612319]\n",
            "[[0 0 1 ... 1 0 0]\n",
            " [0 1 0 ... 0 0 1]\n",
            " [2 3 0 ... 0 2 1]\n",
            " ...\n",
            " [0 1 0 ... 1 0 0]\n",
            " [1 2 0 ... 0 2 0]\n",
            " [1 2 0 ... 3 2 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqS2WMbqHz8-",
        "colab_type": "code",
        "outputId": "d51fba8b-e2d7-4ae2-b31a-cf19b09f8cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(np.mean(average_pt))\n",
        "print(np.mean(pdg_number).ravel())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14.555469221796491\n",
            "[2.88727]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgKuUToptRzb",
        "colab_type": "code",
        "outputId": "7fa652f4-d394-4063-d689-fa3cd387f456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "feature_map = np.concatenate((np.reshape(average_pt, (average_pt.size, 1)), pdg_number), axis=1)\n",
        "print(feature_map.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8GWu4NXIWPX",
        "colab_type": "code",
        "outputId": "38f4ec74-8aa4-4a9c-e24c-24c534bf9306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(feature_map)\n",
        "_, indices = nbrs.kneighbors(feature_map)\n",
        "#nbrs.kneighbors_graph(feature_map).toarray() #return adjc matrix\n",
        "\n",
        "print(indices)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0 17064 70444 11236 28423]\n",
            " [    1 53533 28137 21976 90660]\n",
            " [    2 99448 28005 32533 74298]\n",
            " ...\n",
            " [99997  3746  8665  4528 15614]\n",
            " [99998 15210 42989 42878 83043]\n",
            " [99999  1754 24424 64312 84631]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5eljrhXYts4",
        "colab_type": "text"
      },
      "source": [
        "## Pytorch Geometric Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JEuDVPGY6AA",
        "colab_type": "code",
        "outputId": "1b614691-ce1f-4696-a580-535f4c9a058f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch; print(torch.__version__)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAXikIMdYEZM",
        "colab_type": "code",
        "outputId": "92580b04-e87d-4da5-a12a-56c14fae36b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install --upgrade --force-reinstall torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
            "Collecting torch-scatter==latest+cu101\n",
            "  Using cached https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.4.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-scatter\n",
            "  Found existing installation: torch-scatter 2.0.4\n",
            "    Uninstalling torch-scatter-2.0.4:\n",
            "      Successfully uninstalled torch-scatter-2.0.4\n",
            "Successfully installed torch-scatter-2.0.4\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
            "Collecting torch-sparse==latest+cu101\n",
            "  Using cached https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.4.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Collecting scipy\n",
            "  Using cached https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, scipy, torch-sparse\n",
            "  Found existing installation: numpy 1.18.1\n",
            "    Uninstalling numpy-1.18.1:\n",
            "      Successfully uninstalled numpy-1.18.1\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: torch-sparse 0.6.0\n",
            "    Uninstalling torch-sparse-0.6.0:\n",
            "      Successfully uninstalled torch-sparse-0.6.0\n",
            "Successfully installed numpy-1.18.1 scipy-1.4.1 torch-sparse-0.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
            "Collecting torch-spline-conv==latest+cu101\n",
            "  Using cached https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.4.0/torch_spline_conv-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-spline-conv\n",
            "  Found existing installation: torch-spline-conv 1.2.0\n",
            "    Uninstalling torch-spline-conv-1.2.0:\n",
            "      Successfully uninstalled torch-spline-conv-1.2.0\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.6/dist-packages (1.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.8.0)\n",
            "Requirement already satisfied: plyfile in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.7.1)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.16.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.47.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.25.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->torch-geometric) (1.12.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (6.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (45.2.0)\n",
            "Requirement already satisfied: llvmlite>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.14.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (0.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrqx1C2T58B2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94a87e88-ffc5-4e5e-aacf-23deba296a0d"
      },
      "source": [
        "x = np.array([[[i[j], i[j+1], i[j+1], i[j]] for j in range(len(i)-1)] for i in indices]).reshape((800000,2))\n",
        "\n",
        "x[np.logical_and(np.isin(x[:,0],range(10000)) ,np.isin(x[:,1],range(10000)))].shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7886, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EW1I4aDY3uT",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiCYczO5PZCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class QGJetsDataset(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super(QGJetsDataset, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        pass\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['/content/drive/My Drive/QG_jets_0.dataset']\n",
        "\n",
        "    def process(self):\n",
        "        data_list = []\n",
        "        np_indices = np.array([[[i[j], i[j+1], i[j+1], i[j]] \n",
        "                    for j in range(len(i)-1)] for i in indices]).reshape((800000,2))\n",
        "        #edge_index = torch.tensor(np_indices, dtype=torch.long) #the topK(5) nearest nerbours\n",
        "\n",
        "        size_data = 10000\n",
        "        for i in tqdm(range(10)):\n",
        "          x = torch.tensor(features[size_data*i:size_data*(i+1),:,:].reshape((size_data, -1)), dtype=torch.float) # feature of jets\n",
        "          y = torch.tensor(labels[size_data*i:size_data*(i+1)], dtype=torch.float) # labels of jet\n",
        "\n",
        "          sub_edge_index = np_indices[np.logical_and(np.isin(np_indices[:,0],range(size_data)) ,np.isin(np_indices[:,1],range(size_data)))]\n",
        "          edge_index = torch.tensor(sub_edge_index.T, dtype=torch.long) #the topK(5) nearest nerbours\n",
        "\n",
        "          data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "          #print(data)\n",
        "\n",
        "          data_list.append(data)\n",
        "        \n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocJwz4y_JkMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_loader(train_feature, train_lable, test_feature, test_label, batch_size=256):\n",
        "    # define a trainset\n",
        "    trainset = QGJetsDataset(train_feature, train_lable)\n",
        "    print(trainset)\n",
        "    # define a trainloader\n",
        "    trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
        "    # define a testset\n",
        "    testset = QGJetsDataset(test_feature, test_label)\n",
        "    # define a testloader\n",
        "    testloader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return trainloader, testloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kToP5SPZD2I",
        "colab_type": "text"
      },
      "source": [
        "## Network Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FobaM0HAJMLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Sequential as Seq, Linear, ReLU\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
        "\n",
        "\n",
        "class SAGEConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SAGEConv, self).__init__(aggr='max') #  \"Max\" aggregation.\n",
        "        self.fc = torch.nn.Linear(in_channels, out_channels)\n",
        "        self.act = torch.nn.ReLU()\n",
        "        self.update_fc = torch.nn.Linear(in_channels + out_channels, in_channels, bias=False)\n",
        "        self.update_act = torch.nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, edge_index):\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "        edge_index, _ = remove_self_loops(edge_index)\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
        "\n",
        "    def message(self, x_j):\n",
        "        # x_j has shape [E, in_channels]\n",
        "        x_j = self.fc(x_j)\n",
        "        x_j = self.act(x_j)       \n",
        "        return x_j\n",
        "\n",
        "    def update(self, aggr_out, x):\n",
        "        # aggr_out has shape [N, out_channels]\n",
        "        new_embedding = torch.cat([aggr_out, x], dim=1)      \n",
        "        new_embedding = self.update_fc(new_embedding)\n",
        "        new_embedding = self.update_act(new_embedding)\n",
        "        return new_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enssYAYSJl7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch_geometric.nn import TopKPooling\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MPNNet(torch.nn.Module):\n",
        "    def __init__(self, feature_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc0 = torch.nn.Linear(feature_size, 128)\n",
        "\n",
        "        self.conv1 = SAGEConv(128, 128)\n",
        "        #self.pool1 = TopKPooling(128, ratio=0.8)\n",
        "        self.conv2 = SAGEConv(128, 128)\n",
        "        #self.pool2 = TopKPooling(128, ratio=0.8)\n",
        "        self.conv3 = SAGEConv(128, 128)\n",
        "        #self.pool3 = TopKPooling(128, ratio=0.8)\n",
        "        \n",
        "        self.fc1 = torch.nn.Linear(256, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 64)\n",
        "        self.fc3 = torch.nn.Linear(64, 1)\n",
        "        #self.bn1 = torch.nn.BatchNorm1d(128)\n",
        "        #self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "        self.act2 = torch.nn.ReLU()        \n",
        "  \n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.fc0(x)\n",
        "\n",
        "        print(x.shape)\n",
        "        x = x.squeeze(1)        \n",
        "        print(x.shape)\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        print(x.shape)\n",
        "        #print(x, edge_index, batch)\n",
        "        #x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
        "        #x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        print(x.shape)\n",
        "        #x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
        "        #x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "\n",
        "        #x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
        "        #x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        #x = x1 + x2 + x3\n",
        "        print(x.shape)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.act2(x)      \n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = torch.sigmoid(self.fc3(x)).squeeze(1)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F6to9ooddo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, model, lossFunction, optimizer, device, trainloader):\n",
        "    \"\"\"train model using loss_fn and optimizer. When this function is called, model trains for one epoch.\n",
        "    Args:\n",
        "        train_loader: train data\n",
        "        model: prediction model\n",
        "        loss_fn: loss function to judge the distance between target and outputs\n",
        "        optimizer: optimize the loss function\n",
        "        get_grad: True, False\n",
        "    output:\n",
        "        total_loss: loss\n",
        "        average_grad2: average grad for hidden 2 in this epoch\n",
        "        average_grad3: average grad for hidden 3 in this epoch\n",
        "    \"\"\"\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    model.train()     # enter train mode\n",
        "    train_loss = 0    # accumulate every batch loss in a epoch\n",
        "    correct = 0       # count when model' prediction is correct i train set\n",
        "    total = 0         # total number of prediction in train set\n",
        "    for batch_idx, data in enumerate(trainloader):\n",
        "        inputs, targets = data['feature'], data['label']\n",
        "        inputs, targets = inputs.to(device), targets.to(device) # load data to gpu device\n",
        "        inputs, targets = Variable(inputs), Variable(targets)\n",
        "        optimizer.zero_grad()            # clear gradients of all optimized torch.Tensors'\n",
        "        outputs = model(inputs)          # forward propagation return the value of softmax function\n",
        "        #print('target', targets)\n",
        "        targets = targets.squeeze_()\n",
        "        loss = lossFunction(outputs, targets) #compute loss\n",
        "        loss.backward()                  # compute gradient of loss over parameters \n",
        "        optimizer.step()                 # update parameters with gradient descent \n",
        "\n",
        "        train_loss += loss.item()        # accumulate every batch loss in a epoch\n",
        "        _, predicted = outputs.max(1)    # make prediction according to the outputs\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item() # count how many predictions is correct\n",
        "        \n",
        "    print( 'Train loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
        "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    train_loss = train_loss/(batch_idx+1)\n",
        "    \n",
        "    return train_loss\n",
        "    \n",
        "    \n",
        "def test(model, lossFunction, optimizer, device, testloader):\n",
        "    \"\"\"\n",
        "    test model's prediction performance on loader.  \n",
        "    When thid function is called, model is evaluated.\n",
        "    Args:\n",
        "        loader: data for evaluation\n",
        "        model: prediction model\n",
        "        loss_fn: loss function to judge the distance between target and outputs\n",
        "    output:\n",
        "        total_loss\n",
        "        accuracy\n",
        "    \"\"\"\n",
        "    model.eval() #enter test mode\n",
        "    test_loss = 0 # accumulate every batch loss in a epoch\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(testloader):\n",
        "            inputs, targets = data['feature'], data['label']\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            targets = targets.squeeze_()\n",
        "            loss = lossFunction(outputs, targets) #compute loss\n",
        "\n",
        "            test_loss += loss.item() # accumulate every batch loss in a epoch\n",
        "            _, predicted = outputs.max(1) # make prediction according to the outputs\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item() # count how many predictions is correct\n",
        "        # print loss and acc\n",
        "        print('Test Loss: %.3f  | Test Acc: %.3f%% (%d/%d)'\n",
        "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        test_loss = test_loss/(batch_idx+1)\n",
        "        \n",
        "    return test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MD_5bOZdg5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(model, num_epochs, optimizer, trainloader, testloader, device='cpu', lossFunction=nn.CrossEntropyLoss(), lr=0.01):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "    if device == 'cuda':\n",
        "        model = torch.nn.DataParallel(model)\n",
        "        torch.backends.cudnn.benchmark=True\n",
        "\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss_list.append(train(epoch, model, lossFunction, optimizer, device, trainloader))\n",
        "        test_loss_list.append(test(model, lossFunction, optimizer, device, testloader))\n",
        "\n",
        "    return train_loss_list, test_loss_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2QJymzlZK_6",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYQkwG6Rd09a",
        "colab_type": "code",
        "outputId": "8d5ef85e-d7e9-44c9-b67a-6efa4db5700d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "dataset = QGJetsDataset('/content/drive/My Drive')\n",
        "dataset.shuffle()\n",
        "print(dataset)\n",
        "train_loader = DataLoader(dataset[int(0.9*len(dataset)):], batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(dataset[:int(0.9*len(dataset))], batch_size=256, shuffle=True)\n",
        "for data in train_loader:\n",
        "  print(data)\n",
        "  print(data.num_graphs)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QGJetsDataset(10)\n",
            "Batch(batch=[10000], edge_index=[2, 7886], x=[10000, 556], y=[10000])\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoEXDw8qga2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "48e56969-767d-4b59-c6f3-2c49e2ce8d23"
      },
      "source": [
        "np_indices = np.array([[[i[j], i[j+1], i[j+1], i[j]] \n",
        "  for j in range(len(i)-1)] for i in indices]).reshape((800000,2))\n",
        "edge_index = torch.tensor(np_indices.T, dtype=torch.long) #the topK(5) nearest nerbours\n",
        "x = torch.tensor(features.reshape((100000, -1)), dtype=torch.float) # feature of jets\n",
        "y = torch.tensor(labels, dtype=torch.float) # labels of jet\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "print(data)\n",
        "\n",
        "print(data.num_nodes, data.num_edges, data.num_node_features)\n",
        "print(data.contains_isolated_nodes(), data.contains_self_loops(), data.is_directed())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data(edge_index=[2, 800000], x=[100000, 556], y=[100000])\n",
            "100000 800000 556\n",
            "False False False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXNuQNI8owCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "from torch.nn import BatchNorm1d\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc0 = torch.nn.Linear(556, 128)\n",
        "        self.bn0 = BatchNorm1d(128)\n",
        "        self.conv1 = SAGEConv(128, 128)\n",
        "        self.bn1 = BatchNorm1d(128)\n",
        "        self.conv2 = SAGEConv(128, 128)\n",
        "        self.bn2 = BatchNorm1d(128)\n",
        "        self.conv3 = SAGEConv(128, 128)\n",
        "        self.bn3 = BatchNorm1d(128)\n",
        "        self.fc1 = torch.nn.Linear(128, 64)\n",
        "        self.fc2 = torch.nn.Linear(64, 32)\n",
        "        self.fc3 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.fc0(x)\n",
        "        x = self.bn0(x)\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc3(self.fc2(self.fc1(x)))\n",
        "\n",
        "        #print('last layer:', x.view(-1))\n",
        "\n",
        "        out = F.softmax(x, dim=1)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMQVX6Isd6XX",
        "colab_type": "code",
        "outputId": "af03c6bd-1669-4a93-fe76-c69efc4f8e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "num_epochs = 10\n",
        "batch_size = 256\n",
        "lr = 0.01\n",
        "device='cpu'\n",
        "crit = nn.BCELoss()\n",
        "model = Net()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "print(model)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc0): Linear(in_features=556, out_features=128, bias=True)\n",
            "  (bn0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv1): SAGEConv(\n",
            "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (act): ReLU()\n",
            "    (update_fc): Linear(in_features=256, out_features=128, bias=False)\n",
            "    (update_act): ReLU()\n",
            "  )\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): SAGEConv(\n",
            "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (act): ReLU()\n",
            "    (update_fc): Linear(in_features=256, out_features=128, bias=False)\n",
            "    (update_act): ReLU()\n",
            "  )\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): SAGEConv(\n",
            "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (act): ReLU()\n",
            "    (update_fc): Linear(in_features=256, out_features=128, bias=False)\n",
            "    (update_act): ReLU()\n",
            "  )\n",
            "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMONt8aj-MTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(data):\n",
        "    model.train()\n",
        "\n",
        "    loss_all = 0\n",
        "    #for data in train_loader:\n",
        "\n",
        "    data = data.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "\n",
        "    label = data.y.view(-1, 1)\n",
        "\n",
        "    loss = crit(output, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXfkH5ko-bEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "def evaluate(data):\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #for data in loader:\n",
        "\n",
        "        data = data.to(device)\n",
        "        pred = model(data).detach().cpu().numpy()\n",
        "\n",
        "        label = data.y.detach().cpu().numpy()\n",
        "        #print(pred[:20])\n",
        "        #print(label[:20])\n",
        "\n",
        "        predictions.append(pred)\n",
        "        labels.append(label)\n",
        "\n",
        "    predictions = np.hstack(predictions)\n",
        "    labels = np.hstack(labels)\n",
        "    \n",
        "    return roc_auc_score(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUErsuWEUKyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(1, 10):\n",
        "    loss = train(data)\n",
        "    print('Epoch: {:03d}, Loss: {:.5f}}'.\n",
        "          format(epoch, loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtE1XCLD-e8Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "c322becd-0994-42cb-c221-b2b229422456"
      },
      "source": [
        "for epoch in range(1, 10):\n",
        "    loss = train(data)\n",
        "    train_acc = evaluate(data)\n",
        "    val_acc = evaluate(data)    \n",
        "    test_acc = evaluate(data)\n",
        "    print('Epoch: {:03d}, Loss: {:.5f}, Train Auc: {:.5f}, Val Auc: {:.5f}, Test Auc: {:.5f}'.\n",
        "          format(epoch, loss, train_acc, val_acc, test_acc))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 13.91266, Train Auc: 0.50000, Val Auc: 0.50000, Test Auc: 0.50000\n",
            "Epoch: 002, Loss: 13.91266, Train Auc: 0.50000, Val Auc: 0.50000, Test Auc: 0.50000\n",
            "Epoch: 003, Loss: 13.91266, Train Auc: 0.50000, Val Auc: 0.50000, Test Auc: 0.50000\n",
            "Epoch: 004, Loss: 13.91266, Train Auc: 0.50000, Val Auc: 0.50000, Test Auc: 0.50000\n",
            "Epoch: 005, Loss: 13.91266, Train Auc: 0.50000, Val Auc: 0.50000, Test Auc: 0.50000\n",
            "Epoch: 006, Loss: 13.91266, Train Auc: 0.50000, Val Auc: 0.50000, Test Auc: 0.50000\n",
            "Epoch: 007, Loss: 13.91266, Train Auc: 0.50000, Val Auc: 0.50000, Test Auc: 0.50000\n",
            "Epoch: 008, Loss: 13.91266, Train Auc: 0.50000, Val Auc: 0.50000, Test Auc: 0.50000\n",
            "Epoch: 009, Loss: 13.91266, Train Auc: 0.50000, Val Auc: 0.50000, Test Auc: 0.50000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}